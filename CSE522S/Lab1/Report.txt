##Requirements:
Server will send contents of fragmented files to a number of clients. Each client will sort that fragment of the file based on line numbers and send it back to server.
Server will then merge the fragments back to recreate the complete file.

##Server Design:
    ###File I/O: Server will read each fragmented file into its own buffer. Once all the files are read into their buffer, server will start sending them to the clients
    in a multiplexed way. This deisgn is considered so that socket read and write I/O can happen without waiting for any processing time.
    An alternate design consideration could have been to read each line and immediately send it. However, functions like fgets() that can retrieve full lines may not 
    work on variable length lines. Also, function like getline() is not ANSI standard and may not be available in all Linux distributions.

    ###Socket read/write multiplexing: A single poll() function will be used to multiplex server listening for clients, server sending data to clients and server 
    receiving data from clients.
    The file descriptor(FD) list for poll() will initially have only the server socket - in order words, will only poll for input on that FD. Once a client connects, the
    server will add FD for that client to the poll list and set the event to POLLOUT. Basically, for each client that makes connection, their FD will be added to the list
    and events will be set to POLLOUT. Server will then start sending data to clients that have made connection thus far. Once a particular client is sent all the data,
    the event will be flipped to POLLIN to receive data back from that client. One client could be in POLLOUT state to be sent data while another could be in POLLIN state
    to receive data.
    Code from previous studios is reused. The create_socket() and accept_connection() functions from socket_server_inet_lab1.h is used.

    ###Client data processing: The server will read all the data received from client into its own buffer. If there is 4 clients (4 file fragments), data from each will
    be read into 4 separate buffers. Once all the buffers are ready, they will be converted to 4 separate AVL tree. The individual AVL trees will already be in sorted 
    in order. The trees will then be merged to create one single tree that contains the entire content of the complete file. Each node of the AVL tree will be of the 
    type struct { int line_number; char* line }. Char* line will contain the entire line including the line number. Once a complete AVL tree is formed, char* line will
    be written to a file.

    ###Buffer processing: A function read_fd() is provided to read the entire content of a file into a buffer. It uses read() funciton and provides a way to hold relative
    pointer locations.
    There is a need for the server to parse through the buffer and retrieve each line (or stream of char until and include the line feed). This is
    accomplished by the function parse_buffer(). This arguments to this function are the buffer, a character pointer (say, p_buffer) to store the line, length of the 
    buffer and an int var to hold the relative pointer location. The function will search for '\n' from the beginning of the buffer. Once it finds '\n', it will count 
    the number of bytes to that address from the beginning. It will than copy those memory locations to p_buffer. Var will hold the next position to start searching
    from on the second iteration. The function will be called repeatedly (not recursively) until each line is retreived and processed.

##Client Design:
    Similar to the server, a given client will read the entire data it receives into a buffer. Then it will call a function sort_buffer_AVL() to parse through the buffer
    and create an AVL tree. Since the server is sending a chunck of bytes rather than 'lines', the design consideration is to read all of it into a buffer instead of 
    constructing an AVL tree each time a chunck of data is read. This also minimizes socket I/O time.
    Once a complete AVL tree is created, it is traversed in-order and the content of each node is writted to a send buffer. The client will then send the content of this
    buffer to the server.

##General Design Considerations:
    Both the server and client does reads and writes 1456 bytes at a time. This is the max number of bytes that can be sent over a network without fragmentation. Both
    the server and client extensively uses buffers. This allows for least amount of wait time during socket communication, but is more memory intensive.

##Build Process:
    Extract the contents of the zip file to a location of your choice. Copy the file fragments along with the spec file to the 'server' folder. Run make in the server
    folder to create the server executable. Run make on the client folder to create the client executable. 
    From the server directory, run the server with the following syntax:
    server_lab1 <spec file name> <port>
    From a separate terminal and from the client directory, run the client with the following syntax:
    socket_client_inet_lab1 <IP address> <port>
    There is a script to aid in running multiple clients.
    For localhost, please specify the IP address as 127.0.0.1. Make sure to use a port that is available. Also note that the server and client uses some common code. So
    relative file paths as provided in the tar file needs to be maintained.
    The compressed folder contains a set of test files.

##Testing:
    1) Tested with jabberwocky* files provided with requirements. Recreates the final file correctly.
    2) Took server_lab1.c and cut into 5 files using the provided file_shuffle_cut.cpp program
    3) The final file created was server_lab1_complete. Used the following sed command to strip off line numbers.
        cat server_lab1_complete | sed 's/^[0-9]*//' > server_lab1_nonum
    4) Compiled the code re-generated from server code and ran it against jabberwocky* files. It successfully recrated the file
    
    ####Testing Issues:
    Most of the testing issues were with memory corruption. These were very hard to track down, especially with a larger file that is split up. GDB and print statements
    points out where a malloc is failing or where a segmentation fault is occuring. But the root of the problem does not lie in that particular statement. A good amount
    of effort was put to make sure proper memory allocation is taking place and is freed when no longer needed. The final strategy employed was to allocate a reasonable
    amount of memory and re-allocate twice that amount if more is needed. This made the application a bit more memory intensive, but with the given deadline, it is 
    reasonably successfully. There may still be some buffer overflow issues lingering, but none surfaced with the amount of testing done.

##Development Effort:
     On average 3 hrs per day for 12 days was put towards the project. Roughly 50% of the time was spent debugging memory and buffer issues.